# -*- coding: utf-8 -*-
"""project.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1yM-F5Bplz13lss7BeMjlncbibYnW20Qg
"""

import pandas as pd
import numpy as np
from sklearn.model_selection import KFold
from sklearn.model_selection import cross_val_score
from sklearn import metrics
from sklearn.tree import DecisionTreeClassifier
from sklearn import tree
from sklearn.model_selection import train_test_split
from sklearn.model_selection import GridSearchCV
from matplotlib import pyplot as plt
from sklearn.ensemble import AdaBoostClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.naive_bayes import ComplementNB
from sklearn import svm
from sklearn import preprocessing
from sklearn.preprocessing import LabelEncoder
from sklearn.impute import KNNImputer
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestRegressor

!pip install scikeras

from xgboost import XGBClassifier
from xgboost import plot_importance

import tensorflow as tf
from keras.models import Sequential
from keras.layers import Dense
from keras.utils import to_categorical
from keras.optimizers import SGD
from scikeras.wrappers import KerasClassifier

from imblearn.over_sampling import SMOTE

from google.colab import files
uploaded = files.upload()

import io
data = pd.read_csv(io.BytesIO(uploaded["FINAL Animal Data 2022.csv"]))

#drop unnecessary columns
data.drop(["SubjectID","Run"],axis=1,inplace=True)
oversample = SMOTE()

#plot class distributions
groups = ['control','91db','96db']
group_len  = [len(data[(data["Group"]=="control")]),len(data[(data["Group"]=="91db")]),len(data[(data["Group"]=="96db")])]
plt.bar(x= groups,height = group_len)

#plot class distributions
groups = ['control,70_70','control,70_90','control,90_90','91db,70_70','91db,70_90','91db,90_90','96db,70_70','96db,70_90','96db,90_90']
group_len  = [len(data[(data["Group"]=="control") & (data["Levels"]=="70_70")]),len(data[(data["Group"]=="control") & (data["Levels"]=="70_90")]),len(data[(data["Group"]=="control") & (data["Levels"]=="90_90")]),len(data[(data["Group"]=="91db") & (data["Levels"]=="70_70")]),len(data[(data["Group"]=="91db") & (data["Levels"]=="70_90")]),len(data[(data["Group"]=="91db") & (data["Levels"]=="90_90")]),
             len(data[(data["Group"]=="96db") & (data["Levels"]=="70_70")]),len(data[(data["Group"]=="96db") & (data["Levels"]=="70_90")]),len(data[(data["Group"]=="96db") & (data["Levels"]=="90_90")])]
plt.xticks(fontsize = 6)
plt.bar(x= groups,height = group_len,width = 0.3)

#find rows with missing values
data[data.isnull().any(axis=1)]

#order the variables with the most missing values
order = []
for i in data.columns[data.isna().any()]:
    order.append((data[i].isna().sum(),i))
order.sort(reverse=True)
print(order)

"""#impute for train and test separately.

"""

#imputting process using KNN
train, test = train_test_split(data, test_size=0.2, random_state=42)
data_add_train = pd.DataFrame(columns=data.columns)
data_add_test = pd.DataFrame(columns=data.columns)
for i in ["control","91db","96db"]:
    for j in ["70_70","70_90","90_90"]:
        group_data = train[(train["Group"]==i) & (train["Levels"]==j)].iloc[:,2:]
        imputer = KNNImputer(n_neighbors=6)
        temp = train[(train["Group"]==i) & (train["Levels"]==j)].iloc[:,:2]
        temp = temp.reset_index(drop=True)
        temp = pd.concat([temp,pd.DataFrame(imputer.fit_transform(group_data),columns = train.columns[2:])],axis=1)
        data_add_train = pd.concat([data_add_train,temp],axis=0)
        group_data = test[(test["Group"]==i) & (test["Levels"]==j)].iloc[:,2:]
        if(len(group_data)>0):
          imputer = KNNImputer(n_neighbors=6)
          temp = test[(test["Group"]==i) & (test["Levels"]==j)].iloc[:,:2]
          temp = temp.reset_index(drop=True)
          temp = pd.concat([temp,pd.DataFrame(imputer.fit_transform(group_data),columns = test.columns[2:])],axis=1)
          data_add_test = pd.concat([data_add_test,temp],axis=0)

#plot class distributions for train dataset
groups = ['control,70_70','control,70_90','control,90_90','91db,70_70','91db,70_90','91db,90_90','96db,70_70','96db,70_90','96db,90_90']
group_len  = [len(train[(train["Group"]=="control") & (train["Levels"]=="70_70")]),len(train[(train["Group"]=="control") & (train["Levels"]=="70_90")]),len(train[(train["Group"]=="control") & (train["Levels"]=="90_90")]),len(train[(train["Group"]=="91db") & (train["Levels"]=="70_70")]),len(train[(train["Group"]=="91db") & (train["Levels"]=="70_90")]),len(train[(train["Group"]=="91db") & (train["Levels"]=="90_90")]),
             len(train[(train["Group"]=="96db") & (train["Levels"]=="70_70")]),len(train[(train["Group"]=="96db") & (train["Levels"]=="70_90")]),len(train[(train["Group"]=="96db") & (train["Levels"]=="90_90")])]
plt.xticks(fontsize = 6)
plt.bar(x= groups,height = group_len,width = 0.3)

#plot class distributions for test dataset
groups = ['control,70_70','control,70_90','control,90_90','91db,70_70','91db,70_90','91db,90_90','96db,70_70','96db,70_90','96db,90_90']
group_len  = [len(test[(test["Group"]=="control") & (test["Levels"]=="70_70")]),len(test[(test["Group"]=="control") & (test["Levels"]=="70_90")]),len(test[(test["Group"]=="control") & (test["Levels"]=="90_90")]),len(test[(test["Group"]=="91db") & (test["Levels"]=="70_70")]),len(test[(test["Group"]=="91db") & (test["Levels"]=="70_90")]),len(test[(test["Group"]=="91db") & (test["Levels"]=="90_90")]),
             len(test[(test["Group"]=="96db") & (test["Levels"]=="70_70")]),len(test[(test["Group"]=="96db") & (test["Levels"]=="70_90")]),len(test[(test["Group"]=="96db") & (test["Levels"]=="90_90")])]
plt.xticks(fontsize = 6)
plt.bar(x= groups,height = group_len,width = 0.3)

"""Blue: control group

Orange: 91db group

Green: 96db group
"""

#plot values distributions for each vars in train dataset
g_c = data_add_train[(data_add_train["Group"]=="control")]
g_91 = data_add_train[(data_add_train["Group"]=="91db")]
g_96 = data_add_train[(data_add_train["Group"]=="96db")]
fig, axes = plt.subplots(nrows=10, ncols=10,figsize=(20,25))
for i in range(100):
  axes[int(i/10),i%10].scatter(x=range(len(g_c.iloc[:,i+2])),y = g_c.iloc[:,i+2],s=8,color = 'tab:blue')
  axes[int(i/10),i%10].scatter(x=range(len(g_91.iloc[:,i+2])),y = g_91.iloc[:,i+2],s=8,color = 'tab:orange')
  axes[int(i/10),i%10].scatter(x=range(len(g_96.iloc[:,i+2])),y = g_96.iloc[:,i+2],s=8,color = 'tab:green')
  axes[int(i/10),i%10].set_title(data_add_train.columns[i+2],fontsize = 8)

#plot values distributions for each vars in test dataset
g_c = data_add_test[(data_add_test["Group"]=="control")]
g_91 = data_add_test[(data_add_test["Group"]=="91db")]
g_96 = data_add_test[(data_add_test["Group"]=="96db")]
fig, axes = plt.subplots(nrows=10, ncols=10,figsize=(20,25))
for i in range(100):
  axes[int(i/10),i%10].scatter(x=range(len(g_c.iloc[:,i+2])),y = g_c.iloc[:,i+2],s=8,color = 'tab:blue')
  axes[int(i/10),i%10].scatter(x=range(len(g_91.iloc[:,i+2])),y = g_91.iloc[:,i+2],s=8,color = 'tab:orange')
  axes[int(i/10),i%10].scatter(x=range(len(g_96.iloc[:,i+2])),y = g_96.iloc[:,i+2],s=8,color = 'tab:green')
  axes[int(i/10),i%10].set_title(data_add_test.columns[i+2],fontsize = 8)

le = LabelEncoder()
#y_train: 0-91db, 1-96db, 2-control
X_train,y_train,X_test,y_test = data_add_train.iloc[:,1:],le.fit_transform(data_add_train.iloc[:,0]),data_add_test.iloc[:,1:],le.fit_transform(data_add_test.iloc[:,0])

#get dummies for the categorical values stimulus level
X_train = pd.concat([pd.get_dummies(X_train.iloc[:,0]),X_train.iloc[:,1:]],axis=1)
X_test = pd.concat([pd.get_dummies(X_test.iloc[:,0]),X_test.iloc[:,1:]],axis=1)

#define Kfolds
kfolds = KFold(5,shuffle=True,random_state=1)

#store accuracies and f1 scores
Accuracies_all = []
F1_all = []

"""For each algorithm, following these steps:

1. tune the paramters with grids

2. list each model's paramters

3. plot each model's performance(score)

4. fit the best model with train dataset, and then train the X_test

5. print out the accuracy and F1 score for train dataset and test dataset

(6.) some models can plot the variances importances. If possible,plot them

There are many duplicated codes for scaling and oversampling:

scaling the data: scaler = preprocessing.StandardScaler()

oversampling the train set : X, y = oversample.fit_resample(X_train_scaled,y_train)

def f_importances function shows up several times in order to plot the coefficients of features in svm and logistic algorithm.

## **XGBoost Algorithm**
"""

#tune the paramters with grids
grid = {
    'max_depth': [2,4,6,9],
    'n_estimators':[50,100,150],
    'learning_rate':[0.3,0.5,1],
    'reg_alpha':[0,0.01,0.1,0.5],
    'reg_lambda':[0,0.1,1]
}
models = GridSearchCV(estimator=XGBClassifier(), param_grid=grid, cv= kfolds)
models.fit(X_train, y_train)
models.best_params_

#list every model's parameters
models.cv_results_['params']

#plot the scores(performance) for each model
plt.bar(range(len(models.cv_results_['mean_test_score'])),models.cv_results_['mean_test_score'])

#fit the best model with train dataset, and then train the X_test
model = XGBClassifier(max_depth = 4, learning_rate = 0.5, n_estimators= 50,reg_lambda = 1,reg_alpha = 0)
model.fit(X_train, y_train)
y_pred = model.predict(X_test)
accuracy  = metrics.accuracy_score(y_test, y_pred)
f1 = metrics.f1_score(y_test,y_pred,average = 'weighted')
print("Accuracy on train Dataset: ",metrics.accuracy_score(y_train, model.predict(X_train)))
print("f1 score on train Dataset: ",metrics.f1_score(y_train, model.predict(X_train),average = 'weighted'))
print("Accuracy on test Dataset: ",accuracy)
print("f1 score on test Dataset: ",f1)
print(metrics.classification_report(y_test,y_pred))
Accuracies_all.append(accuracy)
F1_all.append(f1)
importances = model.feature_importances_
indices = np.argsort(importances)
fig, ax = plt.subplots(figsize=(10,20))
ax.barh(range(len(importances)), importances[indices])
ax.set_yticks(range(len(importances)))
ax.tick_params(axis='y', labelsize=5)
_ = ax.set_yticklabels(np.array(X_train.columns)[indices])

"""Compare our model with default parameter"""

model = XGBClassifier()
model.fit(X_train, y_train)
y_pred = model.predict(X_test)
accuracy = metrics.accuracy_score(y_test, y_pred)
f1 = metrics.f1_score(y_test,y_pred,average = 'weighted')
print("Accuracy on train Dataset: ",metrics.accuracy_score(y_train, model.predict(X_train)))
print("f1 score on train Dataset: ",metrics.f1_score(y_train, model.predict(X_train),average = 'weighted'))
print("Accuracy on test Dataset: ",accuracy)
print("f1 score on test Dataset: ",f1)
print(metrics.classification_report(y_test,y_pred))

"""## **Adaboost**
same steps as XGBoost
"""

grid = {
    'algorithm': ["SAMME","SAMME.R"],
    'n_estimators':[10,30,50,70,100,150],
    'learning_rate':[0.5,1,1.5,2]
}
models = GridSearchCV(estimator = AdaBoostClassifier(), param_grid=grid, cv= kfolds)
models.fit(X_train, y_train)
print(models.best_params_,models.best_score_)

models.cv_results_['params']

plt.bar(range(len(models.cv_results_['mean_test_score'])),models.cv_results_['mean_test_score'])

model = AdaBoostClassifier(algorithm = 'SAMME', learning_rate = 1, n_estimators= 100)
model.fit(X_train, y_train)
y_pred = model.predict(X_test)
accuracy = metrics.accuracy_score(y_test, y_pred)
f1 = metrics.f1_score(y_test,y_pred,average = 'weighted')
# print("Accuracy: ",accuracy)
# print("f1 score: ",f1)
print("Accuracy on train Dataset: ",metrics.accuracy_score(y_train, model.predict(X_train)))
print("f1 score on train Dataset: ",metrics.f1_score(y_train, model.predict(X_train),average = 'weighted'))
print("Accuracy on test Dataset: ",accuracy)
print("f1 score on test Dataset: ",f1)
print(metrics.classification_report(y_test,y_pred))
Accuracies_all.append(accuracy)
F1_all.append(f1)
importances = model.feature_importances_
indices = np.argsort(importances)
fig, ax = plt.subplots(figsize=(10,20))
ax.barh(range(len(importances)), importances[indices])
ax.set_yticks(range(len(importances)))
ax.tick_params(axis='y', labelsize=5)
_ = ax.set_yticklabels(np.array(X_train.columns)[indices])

"""## **Naive Bayes**

Naive Bayes, including three sub models:

1. Not dealing with imbalance issue using GaussianNB

2. dealing with imbalance issue using GaussianNB

3. ComplementNB
"""

#these priors are using evenly distributed possibility or the class possibility in train daatset
grid = {
    'priors': [None,[0.34,0.33,0.33],[len(y_train[y_train==[0]])/len(y_train),len(y_train[y_train==[1]])/len(y_train),len(y_train[y_train==[2]])/len(y_train)],[len(y_train[y_train==[2]])/len(y_train),len(y_train[y_train==[0]])/len(y_train),len(y_train[y_train==[1]])/len(y_train)]]
}
models = GridSearchCV(estimator = GaussianNB(), param_grid=grid, cv= kfolds)
scaler = preprocessing.StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)
models.fit(X_train_scaled, y_train)
print(models.best_params_,models.best_score_)

plt.bar(range(len(models.cv_results_['mean_test_score'])),models.cv_results_['mean_test_score'])

#not using SMOTE
model = GaussianNB(priors = [0.34, 0.33, 0.33])
model.fit(X_train_scaled, y_train)
y_pred = model.predict(X_test_scaled)
accuracy = metrics.accuracy_score(y_pred, y_test)
f1 = metrics.f1_score(y_pred, y_test,average="weighted")
print("Accuracy on train Dataset: ",metrics.accuracy_score(y_train, model.predict(X_train_scaled)))
print("f1 score on train Dataset: ",metrics.f1_score(y_train, model.predict(X_train_scaled),average = 'weighted'))
print("Accuracy on test Dataset: ",accuracy)
print("f1 score on test Dataset: ",f1)
print(metrics.classification_report(y_test,y_pred))

#Using Smote to deal with imbalance issue
grid = {
    'priors': [None,[0.34,0.33,0.33],[len(y_train[y_train==[0]])/len(y_train),len(y_train[y_train==[1]])/len(y_train),len(y_train[y_train==[2]])/len(y_train)],[len(y_train[y_train==[2]])/len(y_train),len(y_train[y_train==[0]])/len(y_train),len(y_train[y_train==[1]])/len(y_train)]]
}
models = GridSearchCV(estimator = GaussianNB(), param_grid=grid, cv= kfolds)
scaler = preprocessing.StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)
X, y = oversample.fit_resample(X_train_scaled,y_train)
models.fit(X, y)
print(models.best_params_,models.best_score_)

models.cv_results_['params']

plt.bar(range(len(models.cv_results_['mean_test_score'])),models.cv_results_['mean_test_score'])

model = GaussianNB(priors = [0.0958904109589041, 0.410958904109589, 0.4931506849315068])
model.fit(X, y)
y_pred = model.predict(X_test_scaled)
accuracy = metrics.accuracy_score(y_pred, y_test)
f1 = metrics.f1_score(y_pred, y_test,average="weighted")
print("Accuracy on train Dataset: ",metrics.accuracy_score(y, model.predict(X)))
print("f1 score on train Dataset: ",metrics.f1_score(y, model.predict(X),average = 'weighted'))
print("Accuracy on test Dataset: ",accuracy)
print("f1 score on test Dataset: ",f1)
Accuracies_all.append(accuracy)
F1_all.append(f1)
print(metrics.classification_report(y_test,y_pred))

"""Compare Gaussian NB with ComplementNB, Complement NB performs worse than Gaussian"""

model = ComplementNB()
X, y = oversample.fit_resample(X_train,y_train)
model.fit(X, y)
y_pred = model.predict(X_test_scaled)
accuracy = metrics.accuracy_score(y_pred, y_test)
f1 = metrics.f1_score(y_pred, y_test,average="weighted")
print("Accuracy on train Dataset: ",metrics.accuracy_score(y, model.predict(X)))
print("f1 score on train Dataset: ",metrics.f1_score(y, model.predict(X),average = 'weighted'))
print("Accuracy on test Dataset: ",accuracy)
print("f1 score on test Dataset: ",f1)
print(metrics.classification_report(y_test,y_pred))

"""## **SVM**"""

scaler = preprocessing.StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)
params_grid = [{'kernel': ['rbf'], 'gamma': [1e-4,1e-3,0.01,0.1,0.5],'C': [0.1,1,1.5,10]},
 {'kernel': ['linear'], 'C': [0.1,1,1.5,10]}]
svm_model = GridSearchCV(svm.SVC(), params_grid, cv=kfolds)
svm_model.fit(X_train_scaled, y_train)

print('Best score for training data:', svm_model.best_score_)
# View the best parameters for the model found using grid search
print('Best C:',svm_model.best_estimator_.C)
print('Best Kernel:',svm_model.best_estimator_.kernel)
print('Best Gamma:',svm_model.best_estimator_.gamma)
model = svm_model.best_estimator_
y_pred = model.predict(X_test_scaled)

svm_model.cv_results_['params']

plt.bar(range(len(svm_model.cv_results_['mean_test_score'])),svm_model.cv_results_['mean_test_score'])

accuracy = metrics.accuracy_score(y_test, y_pred)
f1 = metrics.f1_score(y_test, y_pred, average='weighted')
print("Accuracy on train Dataset: ",metrics.accuracy_score(y_train, model.predict(X_train_scaled)))
print("f1 score on train Dataset: ",metrics.f1_score(y_train, model.predict(X_train_scaled),average = 'weighted'))
print("Accuracy on test Dataset: ",accuracy)
print("f1 score on test Dataset: ",f1)
Accuracies_all.append(accuracy)
F1_all.append(f1)
print(metrics.classification_report(y_test,y_pred))

def f_importances(coef, names):
    imp = coef
    imp,names = zip(*sorted(zip(imp,names)))
    plt.figure(figsize=(10,15))
    plt.barh(range(len(names)), imp, align='center')
    plt.tick_params(axis='y', labelsize=5)
    plt.yticks(range(len(names)), names)
    plt.show()
f_importances(model.coef_[0], X_train.columns)
f_importances(model.coef_[1], X_train.columns)
f_importances(model.coef_[2], X_train.columns)

"""## **Neural Network**"""

in_dim = len(X_train.columns)
oversample = SMOTE()
X, y = oversample.fit_resample(X_train,data_add_train.iloc[:,0])
scaler = preprocessing.StandardScaler()
train_features = scaler.fit_transform(X)
test_features = scaler.transform(X_test)

"""Build model because we cannot tune the NNmodel directly with paramters"""

def NNModel(nodes,layer,actf,optimz):
  model = Sequential()
  for i in range(layer):
    model.add(Dense(100, input_dim = in_dim, activation = actf))
  model.add(Dense(3, activation = 'softmax'))
  if(optimz=="SGD"):
    opt = SGD(learning_rate = 0.01)
  else:
    opt = 'Adam'
  model.compile(loss = 'categorical_crossentropy', optimizer = opt, metrics = ['accuracy'])
  return model

"""use Neurons(50/100),activation function(relu/sigmoid),optimization function(SGD/Adam) to compare the performance manually"""

accuracies = []
model = NNModel(50,3,'relu','SGD')
estimator = KerasClassifier(model, epochs=30, batch_size=16, verbose=0)
results = cross_val_score(estimator, train_features, pd.get_dummies(y), cv=kfolds)
print("*****Model Accuracy: %.2f%% (%.2f%%)" % (results.mean()*100, results.std()*100))
accuracies.append(results.mean())
model = NNModel(50,3,'relu','Adam')
estimator = KerasClassifier(model, epochs=30, batch_size=16, verbose=0)
results = cross_val_score(estimator, train_features, pd.get_dummies(y), cv=kfolds)
print("*****Model Accuracy: %.2f%% (%.2f%%)" % (results.mean()*100, results.std()*100))
accuracies.append(results.mean())
model = NNModel(50,3,'sigmoid','SGD')
estimator = KerasClassifier(model, epochs=30, batch_size=16, verbose=0)
results = cross_val_score(estimator, train_features, pd.get_dummies(y), cv=kfolds)
print("*****Model Accuracy: %.2f%% (%.2f%%)" % (results.mean()*100, results.std()*100))
accuracies.append(results.mean())
model = NNModel(50,3,'sigmoid','Adam')
estimator = KerasClassifier(model, epochs=30, batch_size=16, verbose=0)
results = cross_val_score(estimator, train_features, pd.get_dummies(y), cv=kfolds)
print("*****Model Accuracy: %.2f%% (%.2f%%)" % (results.mean()*100, results.std()*100))
accuracies.append(results.mean())
model = NNModel(100,3,'relu','SGD')
estimator = KerasClassifier(model, epochs=30, batch_size=16, verbose=0)
results = cross_val_score(estimator, train_features, pd.get_dummies(y), cv=kfolds)
print("*****Model Accuracy: %.2f%% (%.2f%%)" % (results.mean()*100, results.std()*100))
accuracies.append(results.mean())
model = NNModel(100,3,'relu','Adam')
estimator = KerasClassifier(model, epochs=30, batch_size=16, verbose=0)
results = cross_val_score(estimator, train_features, pd.get_dummies(y), cv=kfolds)
print("*****Model Accuracy: %.2f%% (%.2f%%)" % (results.mean()*100, results.std()*100))
accuracies.append(results.mean())
model = NNModel(100,3,'sigmoid','SGD')
estimator = KerasClassifier(model, epochs=30, batch_size=16, verbose=0)
results = cross_val_score(estimator, train_features, pd.get_dummies(y), cv=kfolds)
print("*****Model Accuracy: %.2f%% (%.2f%%)" % (results.mean()*100, results.std()*100))
accuracies.append(results.mean())
model = NNModel(100,3,'sigmoid','Adam')
estimator = KerasClassifier(model, epochs=30, batch_size=16, verbose=0)
results = cross_val_score(estimator, train_features, pd.get_dummies(y), cv=kfolds)
print("*****Model Accuracy: %.2f%% (%.2f%%)" % (results.mean()*100, results.std()*100))
accuracies.append(results.mean())

print('Accuracies in order:\n',
      '50 Neurons/layer,relu,SGD\n','50 Neurons/layer,relu,Adam,\n','50 Neurons/layer,sigmoid,SGD\n','50 Neurons/layer,sigmoid,Adam,\n','50 Neurons/layer,relu,SGD\n','50 Neurons/layer,relu,Adam,\n','50 Neurons/layer,sigmoid,SGD\n','50 Neurons/layer,sigmoid,Adam,\n')

plt.bar(range(len(accuracies)),accuracies)

in_dim = len(X_train.columns)
oversample = SMOTE()
X, y = oversample.fit_resample(X_train,y_train)
scaler = preprocessing.StandardScaler()
train_features = scaler.fit_transform(X)
test_features = scaler.transform(X_test)
#build the best model from previous selection
model = Sequential()
model.add(Dense(100, input_dim = in_dim, activation = 'relu'))
model.add(Dense(100, activation = 'relu'))
model.add(Dense(100, activation = 'relu'))
model.add(Dense(3, activation = 'softmax'))
model.compile(loss = 'categorical_crossentropy', optimizer = 'Adam', metrics = ['accuracy'])
model.fit(train_features, pd.get_dummies(y), epochs = 30, batch_size = 16,verbose = 2)
scores = model.evaluate(test_features, pd.get_dummies(y_test))
for i, m in enumerate(model.metrics_names): #print loss and accuracy
    print("\n%s: %.3f"% (m, scores[i]))
y_pred = model.predict(test_features).round() #to get the predicted class for X_test after scaling
f1 = metrics.f1_score(y_pred,pd.get_dummies(y_test),average = 'weighted')
print(f1)
print(metrics.classification_report(pd.DataFrame(y_pred),pd.get_dummies(y_test)))
Accuracies_all.append(scores[1])
F1_all.append(f1)

"""## **Random Forest**"""

grid = {
    'criterion':["squared_error","absolute_error"],
    'max_depth': [2,3,5,7,10],
    'max_features': [3,10,"sqrt"],
    'n_estimators':[50,100,150]
}
model = GridSearchCV(estimator=RandomForestRegressor(), param_grid=grid, cv= kfolds)
X, y = oversample.fit_resample(X_train,y_train)
scaler = preprocessing.StandardScaler()
train_features = scaler.fit_transform(X)
model.fit(train_features, y)
model.best_params_

model.cv_results_['params']

plt.bar(range(len(model.cv_results_['mean_test_score'])),model.cv_results_['mean_test_score'])

model = RandomForestRegressor(criterion = 'squared_error',max_depth = 10,max_features = 10,n_estimators= 150)
X, y = oversample.fit_resample(X_train,y_train)
scaler = preprocessing.StandardScaler()
train_features = scaler.fit_transform(X)
test_features = scaler.transform(X_test)
model.fit(train_features, y)
y_pred = model.predict(test_features)
accuracy = metrics.accuracy_score(y_test, [round(i) for i in y_pred])
f1 = metrics.f1_score(y_test,[round(i) for i in y_pred], average='weighted')
print("Accuracy on Train dataset:", metrics.accuracy_score(y, [round(i) for i in model.predict(train_features)]))
print("F1 on Train dataset:", metrics.f1_score(y, [round(i) for i in model.predict(train_features)],average = 'weighted'))
print("Accuracy on Test dataset:", accuracy)
print("F1 on Test dataset:", f1)
print(metrics.classification_report(y_test,  [round(i) for i in y_pred]))
Accuracies_all.append(accuracy)
F1_all.append(f1)

importances = model.feature_importances_
indices = np.argsort(importances)
fig, ax = plt.subplots(figsize=(10,20))
ax.barh(range(len(importances)), importances[indices])
ax.set_yticks(range(len(importances)))
ax.tick_params(axis='y', labelsize=5)
_ = ax.set_yticklabels(np.array(X_train.columns)[indices])

"""## **Logistic**"""

grid = {
    'penalty': ['l1', 'l2'],
    'C':[0.5,1,1.5],
    'tol':[0.0001,0.001,0.01,0.1]
}
models = GridSearchCV(estimator=LogisticRegression(solver = 'liblinear',multi_class = 'ovr'), param_grid=grid, cv= kfolds)
scaler = preprocessing.StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)
X, y = oversample.fit_resample(X_train_scaled,y_train)
models.fit(X, y)
models.best_params_

models.cv_results_['params']

plt.bar(range(len(models.cv_results_['mean_test_score'])),models.cv_results_['mean_test_score'])

model = LogisticRegression(solver = 'liblinear',multi_class = 'ovr', C =  1, penalty = 'l1', tol = 0.01)
model.fit(X, y)
y_pred = model.predict(X_test_scaled)
accuracy  = metrics.accuracy_score(y_test, y_pred)
f1 = metrics.f1_score(y_test,y_pred,average = 'weighted')
print("Accuracy on train Dataset: ",metrics.accuracy_score(y, model.predict(X)))
print("f1 score on train Dataset: ",metrics.f1_score(y, model.predict(X),average = 'weighted'))
print("Accuracy on test Dataset: ",accuracy)
print("f1 score on test Dataset: ",f1)
print(metrics.classification_report(y_test,y_pred))
Accuracies_all.append(accuracy)
F1_all.append(f1)
def f_importances(coef, names):
    imp = coef
    imp,names = zip(*sorted(zip(imp,names)))
    plt.figure(figsize=(10,15))
    plt.barh(range(len(names)), imp, align='center')
    plt.tick_params(axis='y', labelsize=5)
    plt.yticks(range(len(names)), names)
    plt.show()
f_importances(model.coef_[0], X_train.columns)
f_importances(model.coef_[1], X_train.columns)
f_importances(model.coef_[2], X_train.columns)

"""## **Accuracies and F1 score for each algorithm**"""

algos = ['XGBoost','AdaBoost','NaiveBayes','SVM','NeuralNetwork','RandomForest','Logistic']
plt.figure(figsize=(10,6))
plt.bar(algos,Accuracies_all,width =1,label = 'Accuracies')
plt.bar(algos,F1_all,label = 'F1')
plt.legend()

"""## **What happens if we only use the T2 variables? **
## **Repeat above steps/algorithms with T2 variables(+stimulus level) olnly **
"""

X_train_T2 = pd.concat([X_train.iloc[:,:3],X_train.iloc[:,53:]],axis=1)
X_test_T2 = pd.concat([X_test.iloc[:,:3],X_test.iloc[:,53:]],axis=1)

"""## **XGBoost for T2 only**


"""

grid = {
    'max_depth': [2,4,6,9],
    'n_estimators':[100,150],
    'learning_rate':[0.3,0.5,1],
    'reg_alpha':[0,0.01,0.1,0.5],
    'reg_lambda':[0,0.1,1]
}
models = GridSearchCV(estimator=XGBClassifier(), param_grid=grid, cv= kfolds)
models.fit(X_train_T2, y_train)
models.best_params_

model = XGBClassifier(max_depth = 2, learning_rate = 0.3, n_estimators= 100,reg_lambda =0,reg_alpha = 0.5)
model.fit(X_train_T2, y_train)
y_pred = model.predict(X_test_T2)
accuracy  = metrics.accuracy_score(y_test, y_pred)
f1 = metrics.f1_score(y_test,y_pred,average = 'weighted')
print("Accuracy on train Dataset: ",metrics.accuracy_score(y_train, model.predict(X_train_T2)))
print("f1 score on train Dataset: ",metrics.f1_score(y_train, model.predict(X_train_T2),average = 'weighted'))
print("Accuracy on test Dataset: ",accuracy)
print("f1 score on test Dataset: ",f1)
print(metrics.classification_report(y_test,y_pred))
importances = model.feature_importances_
indices = np.argsort(importances)
fig, ax = plt.subplots(figsize=(10,20))
ax.barh(range(len(importances)), importances[indices])
ax.set_yticks(range(len(importances)))
ax.tick_params(axis='y', labelsize=5)
_ = ax.set_yticklabels(np.array(X_train_T2.columns)[indices])

#Now we compare with default parametrs, our chosen model performs better than de the default model.
model = XGBClassifier()
model.fit(X_train_T2, y_train)
y_pred = model.predict(X_test_T2)
accuracy  = metrics.accuracy_score(y_test, y_pred)
f1 = metrics.f1_score(y_test,y_pred,average = 'weighted')
print("Accuracy on train Dataset: ",metrics.accuracy_score(y_train, model.predict(X_train_T2)))
print("f1 score on train Dataset: ",metrics.f1_score(y_train, model.predict(X_train_T2),average = 'weighted'))
print("Accuracy on test Dataset: ",accuracy)
print("f1 score on test Dataset: ",f1)
print(metrics.classification_report(y_test,y_pred))
importances = model.feature_importances_
indices = np.argsort(importances)
fig, ax = plt.subplots(figsize=(10,20))
ax.barh(range(len(importances)), importances[indices])
ax.set_yticks(range(len(importances)))
ax.tick_params(axis='y', labelsize=5)
_ = ax.set_yticklabels(np.array(X_train_T2.columns)[indices])

"""## **Adaboost for T2 only**"""

grid = {
    'algorithm': ["SAMME","SAMME.R"],
    'n_estimators':[10,30,50,70,100,150],
    'learning_rate':[0.5,1,1.5,2]
}
models = GridSearchCV(estimator = AdaBoostClassifier(), param_grid=grid, cv= kfolds)
models.fit(X_train_T2, y_train)
print(models.best_params_,models.best_score_)

models.cv_results_['params']

plt.bar(range(len(models.cv_results_['mean_test_score'])),models.cv_results_['mean_test_score'])

model = AdaBoostClassifier(algorithm = 'SAMME', learning_rate = 1, n_estimators= 10)
model.fit(X_train_T2, y_train)
y_pred = model.predict(X_test_T2)
accuracy = metrics.accuracy_score(y_test, y_pred)
f1 = metrics.f1_score(y_test,y_pred,average = 'weighted')
print("Accuracy on train Dataset: ",metrics.accuracy_score(y_train, model.predict(X_train_T2)))
print("f1 score on train Dataset: ",metrics.f1_score(y_train, model.predict(X_train_T2),average = 'weighted'))
print("Accuracy on test Dataset: ",accuracy)
print("f1 score on test Dataset: ",f1)
print(metrics.classification_report(y_test,y_pred))
importances = model.feature_importances_
indices = np.argsort(importances)
fig, ax = plt.subplots(figsize=(10,20))
ax.barh(range(len(importances)), importances[indices])
ax.set_yticks(range(len(importances)))
ax.tick_params(axis='y', labelsize=5)
_ = ax.set_yticklabels(np.array(X_train_T2.columns)[indices])

"""## **GaussianNB for T2 only**"""

grid = {
    'priors': [None,[0.34,0.33,0.33],[len(y_train[y_train==[0]])/len(y_train),len(y_train[y_train==[1]])/len(y_train),len(y_train[y_train==[2]])/len(y_train)],[len(y_train[y_train==[2]])/len(y_train),len(y_train[y_train==[0]])/len(y_train),len(y_train[y_train==[1]])/len(y_train)]]
}
models = GridSearchCV(estimator = GaussianNB(), param_grid=grid, cv= kfolds)
scaler = preprocessing.StandardScaler()
X_train_T2_scaled = scaler.fit_transform(X_train_T2)
X_test_T2_scaled = scaler.transform(X_test_T2)
models.fit(X_train_T2_scaled, y_train)
print(models.best_params_,models.best_score_)

print(models.cv_results_['params'])
plt.bar(range(len(models.cv_results_['mean_test_score'])),models.cv_results_['mean_test_score'])

#not using SMOTE
model = GaussianNB()
model.fit(X_train_T2_scaled, y_train)
y_pred = model.predict(X_test_T2_scaled)
accuracy = metrics.accuracy_score(y_pred, y_test)
f1 = metrics.f1_score(y_pred, y_test,average="weighted")
print("Accuracy on train Dataset: ",metrics.accuracy_score(y_train, model.predict(X_train_T2_scaled)))
print("f1 score on train Dataset: ",metrics.f1_score(y_train, model.predict(X_train_T2_scaled),average = 'weighted'))
print("Accuracy on test Dataset: ",accuracy)
print("f1 score on test Dataset: ",f1)
print(metrics.classification_report(y_test,y_pred))

#using SMOTE
grid = {
    'priors': [None,[0.34,0.33,0.33],[len(y_train[y_train==[0]])/len(y_train),len(y_train[y_train==[1]])/len(y_train),len(y_train[y_train==[2]])/len(y_train)],[len(y_train[y_train==[2]])/len(y_train),len(y_train[y_train==[0]])/len(y_train),len(y_train[y_train==[1]])/len(y_train)]]
}
models = GridSearchCV(estimator = GaussianNB(), param_grid=grid, cv= kfolds)
scaler = preprocessing.StandardScaler()
X_train_T2_scaled = scaler.fit_transform(X_train_T2)
X_test_T2_scaled = scaler.transform(X_test_T2)
oversample = SMOTE()
X, y = oversample.fit_resample(X_train_T2_scaled,y_train)
models.fit(X, y)
print(models.best_params_,models.best_score_)

print(models.cv_results_['params'])
plt.bar(range(len(models.cv_results_['mean_test_score'])),models.cv_results_['mean_test_score'])

model = GaussianNB(priors =  [0.4931506849315068, 0.0958904109589041, 0.410958904109589])
model.fit(X, y)
y_pred = model.predict(X_test_T2_scaled)
accuracy = metrics.accuracy_score(y_pred, y_test)
f1 = metrics.f1_score(y_pred, y_test,average="weighted")
print("Accuracy on train Dataset: ",metrics.accuracy_score(y, model.predict(X)))
print("f1 score on train Dataset: ",metrics.f1_score(y, model.predict(X),average = 'weighted'))
print("Accuracy on test Dataset: ",accuracy)
print("f1 score on test Dataset: ",f1)
print(metrics.classification_report(y_test,y_pred))

"""## **SVM for T2 only**"""

scaler = preprocessing.StandardScaler()
X_train_T2_scaled = scaler.fit_transform(X_train_T2)
X_test_T2_scaled = scaler.transform(X_test_T2)
params_grid = [{'kernel': ['rbf'], 'gamma': [1e-4,1e-3,0.01,0.1,0.5],'C': [0.1,1,1.5,10]},
 {'kernel': ['linear'], 'C': [0.1,1,1.5,10]}]
svm_model = GridSearchCV(svm.SVC(), params_grid, cv=kfolds)
svm_model.fit(X_train_T2_scaled, y_train)

svm_model.cv_results_['params']

plt.bar(range(len(svm_model.cv_results_['mean_test_score'])),svm_model.cv_results_['mean_test_score'])

print('Best score for training data:', svm_model.best_score_)
# View the best parameters for the model found using grid search
print('Best C:',svm_model.best_estimator_.C)
print('Best Kernel:',svm_model.best_estimator_.kernel)
print('Best Gamma:',svm_model.best_estimator_.gamma)
model = svm_model.best_estimator_
y_pred = model.predict(X_test_T2_scaled)

accuracy = metrics.accuracy_score(y_test, y_pred)
f1 = metrics.f1_score(y_test, y_pred, average='weighted')
print("Accuracy on train Dataset: ",metrics.accuracy_score(y_train, model.predict(X_train_T2_scaled)))
print("f1 score on train Dataset: ",metrics.f1_score(y_train, model.predict(X_train_T2_scaled),average = 'weighted'))
print("Accuracy on test Dataset: ",accuracy)
print("f1 score on test Dataset: ",f1)
print(metrics.classification_report(y_test,y_pred))

"""## **Neural Network for T2 only**"""

in_dim = len(X_train_T2.columns)
oversample = SMOTE()
X, y = oversample.fit_resample(X_train_T2,y_train)
scaler = preprocessing.StandardScaler()
train_features = scaler.fit_transform(X)
test_features = scaler.transform(X_test_T2)

accuracies = []
model = NNModel(50,3,'relu','SGD')
estimator = KerasClassifier(model, epochs=30, batch_size=16, verbose=0)
results = cross_val_score(estimator, train_features, pd.get_dummies(y), cv=kfolds)
print("*****Model Accuracy: %.2f%% (%.2f%%)" % (results.mean()*100, results.std()*100))
accuracies.append(results.mean())
model = NNModel(50,3,'relu','Adam')
estimator = KerasClassifier(model, epochs=30, batch_size=16, verbose=0)
results = cross_val_score(estimator, train_features, pd.get_dummies(y), cv=kfolds)
print("*****Model Accuracy: %.2f%% (%.2f%%)" % (results.mean()*100, results.std()*100))
accuracies.append(results.mean())
model = NNModel(50,3,'sigmoid','SGD')
estimator = KerasClassifier(model, epochs=30, batch_size=16, verbose=0)
results = cross_val_score(estimator, train_features, pd.get_dummies(y), cv=kfolds)
print("*****Model Accuracy: %.2f%% (%.2f%%)" % (results.mean()*100, results.std()*100))
accuracies.append(results.mean())
model = NNModel(50,3,'sigmoid','Adam')
estimator = KerasClassifier(model, epochs=30, batch_size=16, verbose=0)
results = cross_val_score(estimator, train_features, pd.get_dummies(y), cv=kfolds)
print("*****Model Accuracy: %.2f%% (%.2f%%)" % (results.mean()*100, results.std()*100))
accuracies.append(results.mean())
model = NNModel(100,3,'relu','SGD')
estimator = KerasClassifier(model, epochs=30, batch_size=16, verbose=0)
results = cross_val_score(estimator, train_features, pd.get_dummies(y), cv=kfolds)
print("*****Model Accuracy: %.2f%% (%.2f%%)" % (results.mean()*100, results.std()*100))
accuracies.append(results.mean())
model = NNModel(100,3,'relu','Adam')
estimator = KerasClassifier(model, epochs=30, batch_size=16, verbose=0)
results = cross_val_score(estimator, train_features, pd.get_dummies(y), cv=kfolds)
print("*****Model Accuracy: %.2f%% (%.2f%%)" % (results.mean()*100, results.std()*100))
accuracies.append(results.mean())
model = NNModel(100,3,'sigmoid','SGD')
estimator = KerasClassifier(model, epochs=30, batch_size=16, verbose=0)
results = cross_val_score(estimator, train_features, pd.get_dummies(y), cv=kfolds)
print("*****Model Accuracy: %.2f%% (%.2f%%)" % (results.mean()*100, results.std()*100))
accuracies.append(results.mean())
model = NNModel(100,3,'sigmoid','Adam')
estimator = KerasClassifier(model, epochs=30, batch_size=16, verbose=0)
results = cross_val_score(estimator, train_features, pd.get_dummies(y), cv=kfolds)
print("*****Model Accuracy: %.2f%% (%.2f%%)" % (results.mean()*100, results.std()*100))
accuracies.append(results.mean())

print('Accuracies in order:\n',
      '50 Neurons/layer,relu,SGD\n','50 Neurons/layer,relu,Adam,\n','50 Neurons/layer,sigmoid,SGD\n','50 Neurons/layer,sigmoid,Adam,\n','50 Neurons/layer,relu,SGD\n','50 Neurons/layer,relu,Adam,\n','50 Neurons/layer,sigmoid,SGD\n','50 Neurons/layer,sigmoid,Adam,\n')

plt.bar(range(len(accuracies)),accuracies)

model = Sequential()
model.add(Dense(100, input_dim = in_dim, activation = 'relu'))
model.add(Dense(100, activation = 'relu'))
model.add(Dense(100, activation = 'relu'))
model.add(Dense(3, activation = 'softmax'))
model.compile(loss = 'categorical_crossentropy', optimizer = 'Adam', metrics = ['accuracy'])
model.fit(train_features, pd.get_dummies(y), epochs = 30, batch_size = 16,verbose = 2)
scores = model.evaluate(test_features, pd.get_dummies(data_add_test.iloc[:,0]))
for i, m in enumerate(model.metrics_names):
    print("\n%s: %.3f"% (m, scores[i]))
y_pred = model.predict(test_features).round()
print(metrics.f1_score(y_pred,pd.get_dummies(y_test),average = 'weighted'))
print(metrics.classification_report(pd.DataFrame(y_pred),pd.get_dummies(y_test)))

"""## **Random Forest for T2 only**"""

grid = {
    'criterion':["squared_error","absolute_error"],
    'max_depth': [2,3,5,7,10],
    'max_features': [3,10,"sqrt"],
    'n_estimators':[50,100,150]
}
model = GridSearchCV(estimator=RandomForestRegressor(), param_grid=grid, cv= kfolds)
X, y = oversample.fit_resample(X_train_T2,y_train)
scaler = preprocessing.StandardScaler()
train_features = scaler.fit_transform(X)
model.fit(train_features, y)
model.best_params_

model.cv_results_['params']

plt.bar(range(len(model.cv_results_['mean_test_score'])),model.cv_results_['mean_test_score'])

model = RandomForestRegressor(criterion = 'squared_error',max_depth = 10,max_features = "sqrt",n_estimators = 150)
X, y = oversample.fit_resample(X_train_T2,y_train)
scaler = preprocessing.StandardScaler()
train_features = scaler.fit_transform(X)
test_features = scaler.transform(X_test_T2)
model.fit(train_features, y)
y_pred = model.predict(test_features)
accuracy = metrics.accuracy_score(y_test, [round(i) for i in y_pred])
f1 = metrics.f1_score(y_test,[round(i) for i in y_pred], average='weighted')
print("Accuracy on Train dataset:", metrics.accuracy_score(y, [round(i) for i in model.predict(train_features)]))
print("F1 on Train dataset:", metrics.f1_score(y, [round(i) for i in model.predict(train_features)],average = 'weighted'))
print("Accuracy on Test dataset:", accuracy)
print("F1 on Test dataset:", f1)
print(metrics.classification_report(y_test,  [round(i) for i in y_pred]))

importances = model.feature_importances_
indices = np.argsort(importances)
fig, ax = plt.subplots(figsize=(10,20))
ax.barh(range(len(importances)), importances[indices])
ax.set_yticks(range(len(importances)))
ax.tick_params(axis='y', labelsize=5)
_ = ax.set_yticklabels(np.array(X_train_T2.columns)[indices])

"""## **Logsitic for T2 only**"""

grid = {
    'penalty': ['l1', 'l2'],
    'C':[0.5,1,1.5],
    'tol':[0.0001,0.001,0.01,0.1]
}
models = GridSearchCV(estimator=LogisticRegression(solver = 'liblinear',multi_class = 'ovr'), param_grid=grid, cv= kfolds)
scaler = preprocessing.StandardScaler()
X_train_T2_scaled = scaler.fit_transform(X_train_T2)
X_test_T2_scaled = scaler.transform(X_test_T2)
X, y = oversample.fit_resample(X_train_T2_scaled,y_train)
models.fit(X, y)
models.best_params_

models.cv_results_['params']

plt.bar(range(len(models.cv_results_['mean_test_score'])),models.cv_results_['mean_test_score'])

model = LogisticRegression(solver = 'liblinear',multi_class = 'ovr', C = 1.5, penalty = 'l1', tol = 0.0001)
model.fit(X, y)
y_pred = model.predict(X_test_T2_scaled)
accuracy  = metrics.accuracy_score(y_test, y_pred)
f1 = metrics.f1_score(y_test,y_pred,average = 'weighted')
print("Accuracy on train Dataset: ",metrics.accuracy_score(y, model.predict(X)))
print("f1 score on train Dataset: ",metrics.f1_score(y, model.predict(X),average = 'weighted'))
print("Accuracy on test Dataset: ",accuracy)
print("f1 score on test Dataset: ",f1)
print(metrics.classification_report(y_test,y_pred))
def f_importances(coef, names):
    imp = coef
    imp,names = zip(*sorted(zip(imp,names)))
    plt.figure(figsize=(10,15))
    plt.barh(range(len(names)), imp, align='center')
    plt.tick_params(axis='y', labelsize=5)
    plt.yticks(range(len(names)), names)
    plt.show()
f_importances(model.coef_[0], X_train_T2.columns)
f_importances(model.coef_[1], X_train_T2.columns)
f_importances(model.coef_[2], X_train_T2.columns)

"""## *What happens if only using T0?*
## *do the same steps/algorithms as above using T0 variables only*
"""

X_train_T0 = X_train.iloc[:,:53]
X_test_T0 = X_test.iloc[:,:53]

"""## **XGboost for T0 only**"""

grid = {
    'max_depth': [2,4,6,9],
    'n_estimators':[50,100,150],
    'learning_rate':[0.3,0.5,1],
    'reg_alpha':[0,0.01,0.1,0.5],
    'reg_lambda':[0,0.1,1]
}
models = GridSearchCV(estimator=XGBClassifier(), param_grid=grid, cv= kfolds)
models.fit(X_train_T0, y_train)
models.best_params_

model = XGBClassifier(max_depth = 6,learning_rate = 0.5, n_estimators= 100,reg_alpha = 0,reg_lambda = 1)
model.fit(X_train_T0, y_train)
y_pred = model.predict(X_test_T0)
accuracy  = metrics.accuracy_score(y_test, y_pred)
f1 = metrics.f1_score(y_test,y_pred,average = 'weighted')
print("Accuracy on train Dataset: ",metrics.accuracy_score(y_train, model.predict(X_train_T0)))
print("f1 score on train Dataset: ",metrics.f1_score(y_train, model.predict(X_train_T0),average = 'weighted'))
print("Accuracy on test Dataset: ",accuracy)
print("f1 score on test Dataset: ",f1)
print(metrics.classification_report(y_test,y_pred))
importances = model.feature_importances_
indices = np.argsort(importances)
fig, ax = plt.subplots(figsize=(10,20))
ax.barh(range(len(importances)), importances[indices])
ax.set_yticks(range(len(importances)))
ax.tick_params(axis='y', labelsize=5)
_ = ax.set_yticklabels(np.array(X_train_T0.columns)[indices])

"""## **Adaboost for T0 only**"""

grid = {
    'algorithm': ["SAMME","SAMME.R"],
    'n_estimators':[10,30,50,70,100,150],
    'learning_rate':[0.5,1,1.5,2]
}
models = GridSearchCV(estimator = AdaBoostClassifier(), param_grid=grid, cv= kfolds)
models.fit(X_train_T0, y_train)
print(models.best_params_,models.best_score_)

model = AdaBoostClassifier(algorithm = 'SAMME', learning_rate = 0.5, n_estimators= 100)
model.fit(X_train_T0, y_train)
y_pred = model.predict(X_test_T0)
accuracy = metrics.accuracy_score(y_test, y_pred)
f1 = metrics.f1_score(y_test,y_pred,average = 'weighted')
# print("Accuracy: ",accuracy)
# print("f1 score: ",f1)
print("Accuracy on train Dataset: ",metrics.accuracy_score(y_train, model.predict(X_train_T0)))
print("f1 score on train Dataset: ",metrics.f1_score(y_train, model.predict(X_train_T0),average = 'weighted'))
print("Accuracy on test Dataset: ",accuracy)
print("f1 score on test Dataset: ",f1)
print(metrics.classification_report(y_test,y_pred))
importances = model.feature_importances_
indices = np.argsort(importances)
fig, ax = plt.subplots(figsize=(10,20))
ax.barh(range(len(importances)), importances[indices])
ax.set_yticks(range(len(importances)))
ax.tick_params(axis='y', labelsize=5)
_ = ax.set_yticklabels(np.array(X_train_T0.columns)[indices])

"""## **Gaussian NB for T0 only**"""

grid = {
    'priors': [None,[0.34,0.33,0.33],[len(y_train[y_train==[0]])/len(y_train),len(y_train[y_train==[1]])/len(y_train),len(y_train[y_train==[2]])/len(y_train)],[len(y_train[y_train==[2]])/len(y_train),len(y_train[y_train==[0]])/len(y_train),len(y_train[y_train==[1]])/len(y_train)]]
}
models = GridSearchCV(estimator = GaussianNB(), param_grid=grid, cv= kfolds)
scaler = preprocessing.StandardScaler()
X_train_T0_scaled = scaler.fit_transform(X_train_T0)
X_test_T0_scaled = scaler.transform(X_test_T0)
X, y = oversample.fit_resample(X_train_T0_scaled,y_train)
models.fit(X, y)
print(models.best_params_,models.best_score_)

model = GaussianNB(priors =  [0.4931506849315068, 0.0958904109589041, 0.410958904109589])
model.fit(X, y)
y_pred = model.predict(X_test_T0_scaled)
accuracy = metrics.accuracy_score(y_pred, y_test)
f1 = metrics.f1_score(y_pred, y_test,average="weighted")
print("Accuracy on train Dataset: ",metrics.accuracy_score(y, model.predict(X)))
print("f1 score on train Dataset: ",metrics.f1_score(y, model.predict(X),average = 'weighted'))
print("Accuracy on test Dataset: ",accuracy)
print("f1 score on test Dataset: ",f1)
print(metrics.classification_report(y_test,y_pred))

"""## **SVM for T0 only**"""

scaler = preprocessing.StandardScaler()
X_train_T0_scaled = scaler.fit_transform(X_train_T0)
X_test_T0_scaled = scaler.transform(X_test_T0)
params_grid = [{'kernel': ['rbf'], 'gamma': [1e-4,1e-3,0.01,0.1,0.5],'C': [0.1,1,1.5,10]},
 {'kernel': ['linear'], 'C': [0.1,1,1.5,10]}]
svm_model = GridSearchCV(svm.SVC(), params_grid, cv=kfolds)
svm_model.fit(X_train_T0_scaled, y_train)

print('Best score for training data:', svm_model.best_score_)
# View the best parameters for the model found using grid search
print('Best C:',svm_model.best_estimator_.C)
print('Best Kernel:',svm_model.best_estimator_.kernel)
print('Best Gamma:',svm_model.best_estimator_.gamma)
model = svm_model.best_estimator_
y_pred = model.predict(X_test_T0_scaled)

accuracy = metrics.accuracy_score(y_test, y_pred)
f1 = metrics.f1_score(y_test, y_pred, average='weighted')
print("Accuracy on train Dataset: ",metrics.accuracy_score(y_train, model.predict(X_train_T0_scaled)))
print("f1 score on train Dataset: ",metrics.f1_score(y_train, model.predict(X_train_T0_scaled),average = 'weighted'))
print("Accuracy on test Dataset: ",accuracy)
print("f1 score on test Dataset: ",f1)
print(metrics.classification_report(y_test,y_pred))

"""## **Neural Network for T0 only**"""

in_dim = len(X_train_T0.columns)
oversample = SMOTE()
X, y = oversample.fit_resample(X_train_T0,y_train)
scaler = preprocessing.StandardScaler()
train_features = scaler.fit_transform(X)
test_features = scaler.transform(X_test_T0)

accuracies = []
model = NNModel(50,3,'relu','SGD')
estimator = KerasClassifier(model, epochs=30, batch_size=16, verbose=0)
results = cross_val_score(estimator, train_features, pd.get_dummies(y), cv=kfolds)
print("*****Model Accuracy: %.2f%% (%.2f%%)" % (results.mean()*100, results.std()*100))
accuracies.append(results.mean())
model = NNModel(50,3,'relu','Adam')
estimator = KerasClassifier(model, epochs=30, batch_size=16, verbose=0)
results = cross_val_score(estimator, train_features, pd.get_dummies(y), cv=kfolds)
print("*****Model Accuracy: %.2f%% (%.2f%%)" % (results.mean()*100, results.std()*100))
accuracies.append(results.mean())
model = NNModel(50,3,'sigmoid','SGD')
estimator = KerasClassifier(model, epochs=30, batch_size=16, verbose=0)
results = cross_val_score(estimator, train_features, pd.get_dummies(y), cv=kfolds)
print("*****Model Accuracy: %.2f%% (%.2f%%)" % (results.mean()*100, results.std()*100))
accuracies.append(results.mean())
model = NNModel(50,3,'sigmoid','Adam')
estimator = KerasClassifier(model, epochs=30, batch_size=16, verbose=0)
results = cross_val_score(estimator, train_features, pd.get_dummies(y), cv=kfolds)
print("*****Model Accuracy: %.2f%% (%.2f%%)" % (results.mean()*100, results.std()*100))
accuracies.append(results.mean())
model = NNModel(100,3,'relu','SGD')
estimator = KerasClassifier(model, epochs=30, batch_size=16, verbose=0)
results = cross_val_score(estimator, train_features, pd.get_dummies(y), cv=kfolds)
print("*****Model Accuracy: %.2f%% (%.2f%%)" % (results.mean()*100, results.std()*100))
accuracies.append(results.mean())
model = NNModel(100,3,'relu','Adam')
estimator = KerasClassifier(model, epochs=30, batch_size=16, verbose=0)
results = cross_val_score(estimator, train_features, pd.get_dummies(y), cv=kfolds)
print("*****Model Accuracy: %.2f%% (%.2f%%)" % (results.mean()*100, results.std()*100))
accuracies.append(results.mean())
model = NNModel(100,3,'sigmoid','SGD')
estimator = KerasClassifier(model, epochs=30, batch_size=16, verbose=0)
results = cross_val_score(estimator, train_features, pd.get_dummies(y), cv=kfolds)
print("*****Model Accuracy: %.2f%% (%.2f%%)" % (results.mean()*100, results.std()*100))
accuracies.append(results.mean())
model = NNModel(100,3,'sigmoid','Adam')
estimator = KerasClassifier(model, epochs=30, batch_size=16, verbose=0)
results = cross_val_score(estimator, train_features, pd.get_dummies(y), cv=kfolds)
print("*****Model Accuracy: %.2f%% (%.2f%%)" % (results.mean()*100, results.std()*100))
accuracies.append(results.mean())

model = Sequential()
#show the prunning by change #nodes, activ function,optimizer(opt/adam)
model.add(Dense(50, input_dim = in_dim, activation = 'relu'))
model.add(Dense(50, activation = 'relu'))
model.add(Dense(50, activation = 'relu'))
model.add(Dense(3, activation = 'softmax'))
model.compile(loss = 'categorical_crossentropy', optimizer = 'Adam', metrics = ['accuracy'])
model.fit(train_features, pd.get_dummies(y), epochs = 30, batch_size = 16,verbose = 2)
scores = model.evaluate(test_features, pd.get_dummies(y_test))
for i, m in enumerate(model.metrics_names):
    print("\n%s: %.3f"% (m, scores[i]))
y_pred = model.predict(test_features).round()
metrics.f1_score(y_pred,pd.get_dummies(y_test),average = 'weighted')
print(metrics.classification_report(pd.DataFrame(y_pred),pd.get_dummies(y_test)))

"""## **Random Forest for T0 only**"""

grid = {
    'criterion':["squared_error","absolute_error"],
    'max_depth': [2,3,5,7,10],
    'max_features': [3,10,"sqrt"],
    'n_estimators':[50,100,150]
}
model = GridSearchCV(estimator=RandomForestRegressor(), param_grid=grid, cv= kfolds)
X, y = oversample.fit_resample(X_train_T0,y_train)
scaler = preprocessing.StandardScaler()
train_features = scaler.fit_transform(X)
model.fit(train_features, y)
model.best_params_

model = RandomForestRegressor(criterion = 'squared_error',max_depth = 7,max_features = "sqrt",n_estimators = 100)
X, y = oversample.fit_resample(X_train_T0,y_train)
scaler = preprocessing.StandardScaler()
train_features = scaler.fit_transform(X)
test_features = scaler.transform(X_test_T0)
model.fit(train_features, y)
y_pred = model.predict(test_features)
accuracy = metrics.accuracy_score(y_test, [round(i) for i in y_pred])
f1 = metrics.f1_score(y_test,[round(i) for i in y_pred], average='weighted')
print("Accuracy on Train dataset:", metrics.accuracy_score(y, [round(i) for i in model.predict(train_features)]))
print("F1 on Train dataset:", metrics.f1_score(y, [round(i) for i in model.predict(train_features)],average = 'weighted'))
print("Accuracy on Test dataset:", accuracy)
print("F1 on Test dataset:", f1)
print(metrics.classification_report(y_test,  [round(i) for i in y_pred]))

"""## **Logistic for T0 only**"""

grid = {
    'penalty': ['l1', 'l2'],
    'C':[0.5,1,1.5],
    'tol':[0.0001,0.001,0.01,0.1]
}
models = GridSearchCV(estimator=LogisticRegression(solver = 'liblinear',multi_class = 'ovr'), param_grid=grid, cv= kfolds)
scaler = preprocessing.StandardScaler()
X_train_T0_scaled = scaler.fit_transform(X_train_T0)
X_test_T0_scaled = scaler.transform(X_test_T0)
X, y = oversample.fit_resample(X_train_T0_scaled,y_train)
models.fit(X, y)
models.best_params_

model = LogisticRegression(solver = 'liblinear',multi_class = 'ovr', C = 1, penalty = 'l2', tol = 0.0001)
model.fit(X, y)
y_pred = model.predict(X_test_T0_scaled)
accuracy  = metrics.accuracy_score(y_test, y_pred)
f1 = metrics.f1_score(y_test,y_pred,average = 'weighted')
print("Accuracy on train Dataset: ",metrics.accuracy_score(y, model.predict(X)))
print("f1 score on train Dataset: ",metrics.f1_score(y, model.predict(X),average = 'weighted'))
print("Accuracy on test Dataset: ",accuracy)
print("f1 score on test Dataset: ",f1)
print(metrics.classification_report(y_test,y_pred))
def f_importances(coef, names):
    imp = coef
    imp,names = zip(*sorted(zip(imp,names)))
    plt.figure(figsize=(10,15))
    plt.barh(range(len(names)), imp, align='center')
    plt.tick_params(axis='y', labelsize=5)
    plt.yticks(range(len(names)), names)
    plt.show()
f_importances(model.coef_[0], X_train_T0.columns)
f_importances(model.coef_[1], X_train_T0.columns)
f_importances(model.coef_[2], X_train_T0.columns)

"""# *What happens to scramble/oob some important variables shown in the previous plot?*
# *Scramble and OOB A_C2W1T2 and compare results for XGBoost and Adaboost*
"""

X_train_scramble = pd.concat([pd.DataFrame(X_train.iloc[:,:54]).reset_index(drop=True),pd.DataFrame(X_train['A_C2W1T2'].sample(frac=1)).reset_index(drop=True),pd.DataFrame(X_train.iloc[:,55:]).reset_index(drop=True)],axis=1)
X_train_oob = X_train.loc[:, X_train.columns != 'A_C2W1T2']
X_test_oob = X_test.loc[:, X_test.columns != 'A_C2W1T2']

"""## **XGboost for Scramble(using all other variables)**"""

model = XGBClassifier()
model.fit(X_train_scramble, y_train)
y_pred = model.predict(X_test)
accuracy  = metrics.accuracy_score(y_test, y_pred)
f1 = metrics.f1_score(y_test,y_pred,average = 'weighted')
print("Accuracy on train Dataset: ",metrics.accuracy_score(y_train, model.predict(X_train_scramble)))
print("f1 score on train Dataset: ",metrics.f1_score(y_train, model.predict(X_train_scramble),average = 'weighted'))
print("Accuracy on test Dataset: ",accuracy)
print("f1 score on test Dataset: ",f1)
print(metrics.classification_report(y_test,y_pred))
importances = model.feature_importances_
indices = np.argsort(importances)
fig, ax = plt.subplots(figsize=(10,20))
ax.barh(range(len(importances)), importances[indices])
ax.set_yticks(range(len(importances)))
ax.tick_params(axis='y', labelsize=5)
_ = ax.set_yticklabels(np.array(X_train_scramble.columns)[indices])

"""## **XGboost for OOB(using all other variables)**"""

model = XGBClassifier()
model.fit(X_train_oob, y_train)
y_pred = model.predict(X_test_oob)
accuracy  = metrics.accuracy_score(y_test, y_pred)
f1 = metrics.f1_score(y_test,y_pred,average = 'weighted')
print("Accuracy on train Dataset: ",metrics.accuracy_score(y_train, model.predict(X_train_oob)))
print("f1 score on train Dataset: ",metrics.f1_score(y_train, model.predict(X_train_oob),average = 'weighted'))
print("Accuracy on test Dataset: ",accuracy)
print("f1 score on test Dataset: ",f1)
print(metrics.classification_report(y_test,y_pred))
importances = model.feature_importances_
indices = np.argsort(importances)
fig, ax = plt.subplots(figsize=(10,20))
ax.barh(range(len(importances)), importances[indices])
ax.set_yticks(range(len(importances)))
ax.tick_params(axis='y', labelsize=5)
_ = ax.set_yticklabels(np.array(X_train_oob.columns)[indices])

"""## **Adaboost for Scramble(using all other variables)**"""

grid = {
    'algorithm': ["SAMME","SAMME.R"],
    'n_estimators':[10,30,50,70,100,150],
    'learning_rate':[0.5,1,1.5,2]
}
models = GridSearchCV(estimator = AdaBoostClassifier(), param_grid=grid, cv= kfolds)
models.fit(X_train_scramble, y_train)
print(models.best_params_,models.best_score_)

model = AdaBoostClassifier(algorithm = 'SAMME', learning_rate = 1, n_estimators= 70)
model.fit(X_train_scramble, y_train)
y_pred = model.predict(X_test)
accuracy = metrics.accuracy_score(y_test, y_pred)
f1 = metrics.f1_score(y_test,y_pred,average = 'weighted')
print("Accuracy on train Dataset: ",metrics.accuracy_score(y_train, model.predict(X_train_scramble)))
print("f1 score on train Dataset: ",metrics.f1_score(y_train, model.predict(X_train_scramble),average = 'weighted'))
print("Accuracy on test Dataset: ",accuracy)
print("f1 score on test Dataset: ",f1)
print(metrics.classification_report(y_test,y_pred))
importances = model.feature_importances_
indices = np.argsort(importances)
fig, ax = plt.subplots(figsize=(10,20))
ax.barh(range(len(importances)), importances[indices])
ax.set_yticks(range(len(importances)))
ax.tick_params(axis='y', labelsize=5)
_ = ax.set_yticklabels(np.array(X_train_scramble.columns)[indices])

"""## **Adaboost for OOB(using all other variables)**"""

grid = {
    'algorithm': ["SAMME","SAMME.R"],
    'n_estimators':[10,30,50,70,100,150],
    'learning_rate':[0.5,1,1.5,2]
}
models = GridSearchCV(estimator = AdaBoostClassifier(), param_grid=grid, cv= kfolds)
models.fit(X_train_oob, y_train)
print(models.best_params_,models.best_score_)

model = AdaBoostClassifier(algorithm = 'SAMME', learning_rate = 1, n_estimators= 70)
model.fit(X_train_oob, y_train)
y_pred = model.predict(X_test_oob)
accuracy = metrics.accuracy_score(y_test, y_pred)
f1 = metrics.f1_score(y_test,y_pred,average = 'weighted')
print("Accuracy on train Dataset: ",metrics.accuracy_score(y_train, model.predict(X_train_oob)))
print("f1 score on train Dataset: ",metrics.f1_score(y_train, model.predict(X_train_oob),average = 'weighted'))
print("Accuracy on test Dataset: ",accuracy)
print("f1 score on test Dataset: ",f1)
print(metrics.classification_report(y_test,y_pred))
importances = model.feature_importances_
indices = np.argsort(importances)
fig, ax = plt.subplots(figsize=(10,20))
ax.barh(range(len(importances)), importances[indices])
ax.set_yticks(range(len(importances)))
ax.tick_params(axis='y', labelsize=5)
_ = ax.set_yticklabels(np.array(X_train_oob.columns)[indices])